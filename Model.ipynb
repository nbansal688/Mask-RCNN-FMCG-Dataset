{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "import random\n",
    "import xmltodict\n",
    "from mrcnn.model import log\n",
    "ROOT_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"../model_saved\"\n",
    "COCO_MODEL_PATH = \"../mask_rcnn_coco.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows = 1, cols = 1, size = 8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize = (size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesConfig(Config):\n",
    "    def __init__(self, num_classes):\n",
    "        self.NUM_CLASSES = 1 + 5\n",
    "        super().__init__()\n",
    "    \"\"\"Configuration for training.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"objects\"\n",
    "    \n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 768\n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 20\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 30\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig(num_classes = 6)\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the objects dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self , folder_name):\n",
    "        \"\"\"Loads all the number of images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"objects\", 1, \"beauty\")\n",
    "        self.add_class(\"objects\", 2, \"venus\")\n",
    "        self.add_class(\"objects\", 3, \"pharmacy\")\n",
    "        self.add_class(\"objects\" , 4 , \"oralcare\")\n",
    "        self.add_class(\"objects\" , 5 , \"reject\")\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for folder in os.listdir(os.path.join(ROOT_DIR , folder_name)):\n",
    "            for file in os.listdir(os.path.join(ROOT_DIR , folder_name , folder)):\n",
    "                extension = os.path.splitext(file)[1]\n",
    "                if extension == \".xml\":\n",
    "                    file = open(os.path.join(ROOT_DIR , folder_name , folder , file))\n",
    "                    dictionary = xmltodict.parse(file.read())\n",
    "                    objects = self.image(dict(dictionary))\n",
    "                    self.add_image(\"objects\" , image_id = i , path = os.path.join(ROOT_DIR , folder_name , folder , dictionary['annotation']['filename']) , width = int(dictionary['annotation']['size']['width']) , height = int(dictionary['annotation']['size']['height']) , depth = int(dictionary['annotation']['size']['depth']) , objects = objects)\n",
    "                    i = i + 1\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Outputs an image from the path of the given image ID.\n",
    "        Typically this function loads the image from a file\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]                         \n",
    "        path = info['path']\n",
    "        image = cv2.imread(path)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the objects data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"objects\":\n",
    "            return info[\"objects\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['objects']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'] , info['width'] , count], dtype = np.uint8)\n",
    "        for i, (objects , pose , truncated , difficult , dims) in enumerate(info['objects']):\n",
    "            xmin , ymin , xmax , ymax = dims\n",
    "            mask[: , : , i:i+1] = cv2.rectangle(mask[: , : , i:i+1].copy() , (xmin, ymin), (xmax, ymax), (255 , 255 , 255) , -1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[: , : , -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[: , :  , i] = mask[: , :  , i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[: , : , i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def objects_values(self, object_data):\n",
    "        \"\"\"Outputs specifications of the objects that lies within\n",
    "        the given image.\n",
    "        Returns a tuple of five valus:\n",
    "        * The object name \n",
    "        * Object Pose\n",
    "        * Object Dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        * Object Difficulty\n",
    "        * Object Truncated or Not\n",
    "        \"\"\"\n",
    "        # Objects\n",
    "        objects = self.classes(object_data['name'])\n",
    "        \n",
    "        # Pose\n",
    "        pose = object_data['pose']\n",
    "        \n",
    "        # Truncated\n",
    "        truncated = object_data['truncated']\n",
    "        \n",
    "        # Difficult\n",
    "        difficult = object_data['difficult']\n",
    "        \n",
    "        # xmin , ymin , xmax , ymax\n",
    "        xmin = int(object_data['bndbox']['xmin'])\n",
    "        ymin = int(object_data['bndbox']['ymin'])\n",
    "        xmax = int(object_data['bndbox']['xmax'])\n",
    "        ymax = int(object_data['bndbox']['ymax'])\n",
    "    \n",
    "        return objects , pose , truncated , difficult ,  (xmin , ymin , xmax , ymax)\n",
    "    def classes(self , class_name):\n",
    "        if class_name == \"w\":\n",
    "            return \"oralcare\"\n",
    "        elif class_name == \"orlab\":\n",
    "            return \"oralcare\"\n",
    "        elif class_name == \"oral\":\n",
    "            return \"oralcare\"\n",
    "        elif class_name == \"oralb\":\n",
    "            return \"oralcare\"\n",
    "        elif class_name == \"vicks\":\n",
    "            return \"pharmacy\"\n",
    "        elif class_name == \"pampers\":\n",
    "            return \"pharmacy\"\n",
    "        elif class_name == \"whisper\":\n",
    "            return \"pharmacy\"\n",
    "        elif class_name == \"gillete\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"gillette\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"ambipureVesus\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"ambipur\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"ambipure\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"home\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"ariel\":\n",
    "            return \"beauty\"\n",
    "        elif class_name == \"olay\":\n",
    "            return \"beauty\"\n",
    "        else:\n",
    "            return class_name\n",
    "\n",
    "    def image(self, annotation_data):\n",
    "        \"\"\"Creates specifications of the image with multiple objects.\n",
    "        Returns a list of object specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        l = 0\n",
    "        if 'object' in annotation_data['annotation']:\n",
    "            for i in annotation_data['annotation']['object']:\n",
    "                if i == \"name\":\n",
    "                    l = l + 1\n",
    "            if l == 0:\n",
    "                N = len(annotation_data['annotation']['object'])\n",
    "                for i in range(N):\n",
    "                    objects , pose , truncated , difficult , dims = self.objects_values(annotation_data['annotation']['object'][i])\n",
    "                    shapes.append((objects , pose , truncated , difficult , dims))\n",
    "                    xmin , ymin , xmax , ymax = dims\n",
    "                    boxes.append([xmin , ymin , xmax , ymax])\n",
    "            elif l == 1 :\n",
    "                objects , pose , truncated , difficult , dims = self.objects_values(annotation_data['annotation']['object'])\n",
    "                shapes.append((objects , pose , truncated , difficult , dims))\n",
    "                xmin , ymin , xmax , ymax = dims\n",
    "                boxes.append([xmin , ymin , xmax , ymax])\n",
    "            # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "            # shapes covering each other\n",
    "            if l == 0:\n",
    "                N = len(annotation_data['annotation']['object'])\n",
    "                keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "                shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        else:\n",
    "            pass\n",
    "        return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(folder_name = \"train/\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_test = ShapesDataset()\n",
    "dataset_test.load_shapes(folder_name = \"test/\")\n",
    "dataset_test.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next code to check some objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for i,m in enumerate(dataset_train.image_info):\n",
    "    info = dataset_train.image_info[i]\n",
    "    objects = info['objects']\n",
    "    for j, (objects , pose , truncated , difficult , dims) in enumerate(info['objects']):\n",
    "        if objects == \"None\":\n",
    "            image = dataset_train.load_image(i)\n",
    "            mask, class_ids = dataset_train.load_mask(i)\n",
    "            visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "            print(info['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode = \"training\" , config = config , model_dir = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_with = \"coco\"  # imagenet, coco\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name = True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    model.load_weights(COCO_MODEL_PATH , by_name = True, exclude = [\"mrcnn_class_logits\" , \"mrcnn_bbox_fc\" , \"mrcnn_bbox\" , \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataset_train , dataset_test , learning_rate = config.LEARNING_RATE , epochs = 1, layers = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running model on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig(num_classes = 6)\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", \n",
    "                          config = inference_config,\n",
    "                          model_dir = MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = random.choice(dataset_test.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_test, inference_config, \n",
    "                           image_id, use_mini_mask = False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize = (8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose = 1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_test.class_names, r['scores'], ax = get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = np.random.choice(dataset_test.image_ids , 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_test, inference_config,\n",
    "                               image_id, use_mini_mask = False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
